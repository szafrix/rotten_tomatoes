INFO 02-24 21:12:36 llm_engine.py:73] Initializing an LLM engine with config: model='BAAI/AquilaChat-7B', tokenizer='BAAI/AquilaChat-7B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:13:39 llm_engine.py:222] # GPU blocks: 615, # CPU blocks: 512
OUTPUT:  
SENTence: the film is a t ry to make a


------------------------------------------------------------------------------------------
OUTPUT:  </s>均不可用。用其他语言输入时，输入法和语言模型无法


------------------------------------------------------------------------------------------
OUTPUT:  
新技术将改变电影行业
新技术将改变电影行业。新技术将


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a tawdry and transparent exercise


------------------------------------------------------------------------------------------
OUTPUT:  
1. </s>均不可行。
2. 不可行。


------------------------------------------------------------------------------------------
INFO 02-24 21:13:42 llm_engine.py:73] Initializing an LLM engine with config: model='baichuan-inc/Baichuan-7B', tokenizer='baichuan-inc/Baichuan-7B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
WARNING 02-24 21:13:43 tokenizer.py:79] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 02-24 21:14:23 llm_engine.py:222] # GPU blocks: 776, # CPU blocks: 512
OUTPUT:  
SENTENCE: the film is a bit of a mess .


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a tedious and uninspired


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess , but


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a masterpiece of suspense and


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a masterpiece of the genre 


------------------------------------------------------------------------------------------
WARNING 02-24 21:14:25 config.py:484] The model's config.json does not contain any of the following keys to determine the original maximum length of the model: ['max_position_embeddings', 'n_positions', 'max_seq_len', 'seq_length', 'max_sequence_length', 'max_seq_length', 'seq_len']. Assuming the model's maximum length is 2048.
INFO 02-24 21:14:25 llm_engine.py:73] Initializing an LLM engine with config: model='tiiuae/falcon-7b', tokenizer='tiiuae/falcon-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:15:04 llm_engine.py:222] # GPU blocks: 45887, # CPU blocks: 32768
OUTPUT:  
SENTENCE: the film is a bit of a mess .


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess ,


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess ,


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess ,


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess ,


------------------------------------------------------------------------------------------
INFO 02-24 21:15:06 llm_engine.py:73] Initializing an LLM engine with config: model='stabilityai/stablelm-tuned-alpha-7b', tokenizer='stabilityai/stablelm-tuned-alpha-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:16:32 llm_engine.py:222] # GPU blocks: 756, # CPU blocks: 682
OUTPUT:  
SENTENCE: the film is a must-see for anyone who


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the movie is a perfect example of how a movie


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the movie is a must-see for any fan


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the movie is a perfect example of how a good


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a commentary on the corruption of the


------------------------------------------------------------------------------------------
INFO 02-24 21:16:35 llm_engine.py:73] Initializing an LLM engine with config: model='internlm/internlm-chat-7b', tokenizer='internlm/internlm-chat-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
WARNING 02-24 21:16:36 tokenizer.py:79] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 02-24 21:17:15 llm_engine.py:222] # GPU blocks: 616, # CPU blocks: 512
OUTPUT:  POSITIVE 


------------------------------------------------------------------------------------------
OUTPUT:  NEGATIVE 


------------------------------------------------------------------------------------------
OUTPUT:  POSITIVE 


------------------------------------------------------------------------------------------
OUTPUT:  POSITIVE 


------------------------------------------------------------------------------------------
OUTPUT:  NEGATIVE 


------------------------------------------------------------------------------------------
INFO 02-24 21:17:17 llm_engine.py:73] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:18:05 llm_engine.py:222] # GPU blocks: 1338, # CPU blocks: 2048
OUTPUT:  
SENTENCE: the filmmaker's heart is in the right


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess , but


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess , but


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess , but


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess , but


------------------------------------------------------------------------------------------
INFO 02-24 21:18:07 llm_engine.py:73] Initializing an LLM engine with config: model='01-ai/Yi-6B', tokenizer='01-ai/Yi-6B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:18:42 llm_engine.py:222] # GPU blocks: 7937, # CPU blocks: 4096
OUTPUT:  NEGATIVE 


------------------------------------------------------------------------------------------
OUTPUT:  NEGATIVE 


------------------------------------------------------------------------------------------
OUTPUT:  POSITIVE 


------------------------------------------------------------------------------------------
OUTPUT:  POSITIVE 


------------------------------------------------------------------------------------------
OUTPUT:  NEGATIVE 


------------------------------------------------------------------------------------------
INFO 02-24 21:18:44 llm_engine.py:73] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:19:20 llm_engine.py:222] # GPU blocks: 881, # CPU blocks: 512
OUTPUT:  
SENTENCE: the film is a bit too long and the story


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a little too long , but it


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit too long and the plot


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a well-crafted , well


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a well-crafted , well


------------------------------------------------------------------------------------------
INFO 02-24 21:19:22 llm_engine.py:73] Initializing an LLM engine with config: model='Qwen/Qwen-7B-Chat', tokenizer='Qwen/Qwen-7B-Chat', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
WARNING 02-24 21:19:23 tokenizer.py:79] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
ERROR: Qwen/Qwen-7B-Chat expected scalar type Half but found BFloat16
