INFO 02-24 21:20:33 llm_engine.py:73] Initializing an LLM engine with config: model='BAAI/AquilaChat-7B', tokenizer='BAAI/AquilaChat-7B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:21:37 llm_engine.py:222] # GPU blocks: 615, # CPU blocks: 512
OUTPUT:  
评估此电影的人对电影有很高的感觉。</s>均不可用。


------------------------------------------------------------------------------------------
OUTPUT:  
评估为负面。</s>均不可用。
评估为积极。可


------------------------------------------------------------------------------------------
OUTPUT:  
S.

節目情節有趣，但缺乏


------------------------------------------------------------------------------------------
OUTPUT:  
评估该电影的情感是积极的。</s>均不可用。</s>均不可用


------------------------------------------------------------------------------------------
OUTPUT:  
评估电影是否优秀需要考虑到很多因素，包括演员表演是否自然


------------------------------------------------------------------------------------------
INFO 02-24 21:21:40 llm_engine.py:73] Initializing an LLM engine with config: model='baichuan-inc/Baichuan-7B', tokenizer='baichuan-inc/Baichuan-7B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
WARNING 02-24 21:21:40 tokenizer.py:79] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 02-24 21:22:20 llm_engine.py:222] # GPU blocks: 776, # CPU blocks: 512
OUTPUT:  

A: I think you're looking for a simple pattern like


------------------------------------------------------------------------------------------
OUTPUT:  
A: I would say that the review is negative.





------------------------------------------------------------------------------------------
OUTPUT:  
A: The sentence is positive.
The reviewer is saying that


------------------------------------------------------------------------------------------
OUTPUT:  

A: You can use the following regex:
^(


------------------------------------------------------------------------------------------
OUTPUT:  
A: The sentence is negative.
The reviewer is saying that


------------------------------------------------------------------------------------------
WARNING 02-24 21:22:22 config.py:484] The model's config.json does not contain any of the following keys to determine the original maximum length of the model: ['max_position_embeddings', 'n_positions', 'max_seq_len', 'seq_length', 'max_sequence_length', 'max_seq_length', 'seq_len']. Assuming the model's maximum length is 2048.
INFO 02-24 21:22:22 llm_engine.py:73] Initializing an LLM engine with config: model='tiiuae/falcon-7b', tokenizer='tiiuae/falcon-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:23:02 llm_engine.py:222] # GPU blocks: 45887, # CPU blocks: 32768
OUTPUT:  
positive

negative

neutral

I'm not sure


------------------------------------------------------------------------------------------
OUTPUT:  
The movie is a bit predictable, but it is still a good movie


------------------------------------------------------------------------------------------
OUTPUT:  
The movie is a good one.

The movie is a bad


------------------------------------------------------------------------------------------
OUTPUT:  
The movie is a thriller.

The movie is a drama.


------------------------------------------------------------------------------------------
OUTPUT:  
REVIEW SENTIMENT: 

REVIEW SENTIM


------------------------------------------------------------------------------------------
INFO 02-24 21:23:03 llm_engine.py:73] Initializing an LLM engine with config: model='stabilityai/stablelm-tuned-alpha-7b', tokenizer='stabilityai/stablelm-tuned-alpha-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:24:30 llm_engine.py:222] # GPU blocks: 756, # CPU blocks: 682
OUTPUT:  
SENTENCE: the film is a must-see . . .


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: so unoriginal and so uninspired that it's


------------------------------------------------------------------------------------------
OUTPUT:  
The review is positive.

Sentence: evokes the style


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: a young woman is kidnapped and held captive in a


------------------------------------------------------------------------------------------
OUTPUT:  
Review Sentiment Analysis is a technique used to determine the overall sentiment of


------------------------------------------------------------------------------------------
INFO 02-24 21:24:32 llm_engine.py:73] Initializing an LLM engine with config: model='internlm/internlm-chat-7b', tokenizer='internlm/internlm-chat-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
WARNING 02-24 21:24:33 tokenizer.py:79] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 02-24 21:25:12 llm_engine.py:222] # GPU blocks: 615, # CPU blocks: 512
OUTPUT:  positive<eoa>



------------------------------------------------------------------------------------------
OUTPUT:  negative

The sentiment of this review is negative.<eoa>



------------------------------------------------------------------------------------------
OUTPUT:  positive<eoa>



------------------------------------------------------------------------------------------
OUTPUT:  positive<eoa>



------------------------------------------------------------------------------------------
OUTPUT:  negative<eoa>



------------------------------------------------------------------------------------------
INFO 02-24 21:25:14 llm_engine.py:73] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:26:02 llm_engine.py:222] # GPU blocks: 1338, # CPU blocks: 2048
OUTPUT:  
SENTENCE: the filmmaker's heart is in the right


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: so verbally flatfooted and so emotionally predict


------------------------------------------------------------------------------------------
OUTPUT:  

















------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess, but


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a fascinating look at the life of


------------------------------------------------------------------------------------------
INFO 02-24 21:26:03 llm_engine.py:73] Initializing an LLM engine with config: model='01-ai/Yi-6B', tokenizer='01-ai/Yi-6B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:26:39 llm_engine.py:222] # GPU blocks: 7937, # CPU blocks: 4096
OUTPUT:  
positive


------------------------------------------------------------------------------------------
OUTPUT:  
negative


------------------------------------------------------------------------------------------
OUTPUT:  
POSITIVE


------------------------------------------------------------------------------------------
OUTPUT:  
positive


------------------------------------------------------------------------------------------
OUTPUT:  
negative


------------------------------------------------------------------------------------------
INFO 02-24 21:26:41 llm_engine.py:73] Initializing an LLM engine with config: model='mosaicml/mpt-7b', tokenizer='mosaicml/mpt-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 02-24 21:27:17 llm_engine.py:222] # GPU blocks: 881, # CPU blocks: 512
OUTPUT:  
SENTENCE: the film is a bit too long . . .


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit of a mess, but


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit too long and the plot


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a bit too long and the plot


------------------------------------------------------------------------------------------
OUTPUT:  
SENTENCE: the film is a fascinating look at the life of


------------------------------------------------------------------------------------------
INFO 02-24 21:27:19 llm_engine.py:73] Initializing an LLM engine with config: model='Qwen/Qwen-7B-Chat', tokenizer='Qwen/Qwen-7B-Chat', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
WARNING 02-24 21:27:20 tokenizer.py:79] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
ERROR: Qwen/Qwen-7B-Chat expected scalar type Half but found BFloat16
